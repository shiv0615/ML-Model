{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "A model to forecast next 12 months of Emergency and Normal service calls.<br>\n", "Data: The dataset provided is related to service calls, specifically when they were received,<br>\n", "the priority, and the town where the service is needed.<br>\n", "The priority is contained within the request_type column and is either an emergency (E) or normal (N).<br>\n", "This data is spread across two tables:<br>\n", "              1. RECEIVED: Date and time when the call was received<br>\n", "              2. REQ_INFO: Request type and town for the call<br>\n", "Data provided as a SQLite DB.<br>\n", "Approach to Problem Solving:<br>\n", "Injest data -> EDA -> Model fit -> Frecast<br>\n", "Refer to README.txt for additional details<br>\n", "For best view, read in PyCharm - sections can be concatenated<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["egion Import"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import seaborn as sns\n", "from EDA import ExploratoryDataAanalysis\n", "from DataIngestion import DataInjestion\n", "from Model import Model\n", "pd.set_option(\"display.max_columns\", 120)\n", "pd.set_option(\"display.max_rows\", 120)\n", "#endregion Import"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#region DefineInput"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["filepath = r'../Data/takehomeDB.db'\n", "skipEDA = False\n", "skipModel = False\n", "LogTranform = False\n", "#endregion DefineInput"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#region Data Injection and Preprocessing<br>\n", "hort Description"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Read in SQLLite db and conver to pandas; merge the data frames. Check for missing values.<br>\n", "Ensure no loss of information by checking the shapes of the df and sql tables.<br>\n", "Merge date and time and convert time to normal datetime format. <br>\n", "Encode E/N for summing the number of requests<br>\n", "Extract/create features for ML/forecast/EDA. <br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DI = DataInjestion(filepath=filepath,tablenames=['RECEIVED','REQ_INFO'])\n", "data = DI.get_data()\n", "#endregion Data Injection and Preprocessing"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# region Exploratory Data Analysis"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Class to sample/aggregate data at different frequency.<br>\n", "Class holds different dataframe and can be accessed via dictionary key.<br>\n", "Idea is not to have large df sporadically distributed and a single<br>\n", "container object helps with a clean implementation. Furthermore, repetitive <br>\n", "plotting calls can be encapsulated by simply passing data frame names as opposed to <br>\n", "passing large dataframes around. Same goes for other classes<br>\n", "-This step is done to understand, transform and plot the data<br>\n", "-Perform basic QA/QC of the data and check for outliers: Request_E & Request_N outliers <br>\n", "-Extract trends, seasonility and error components; perform hypothesis testing for stationarity, trends and seasonality<br>\n", "-Check for correlation between variables to decide between univariate vs multivariate forecasting methods<br>\n", "-Fit simple models (Holt Winters) to extract early patterns <br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["EDA = ExploratoryDataAanalysis(data=data, agg={'REQUEST_TYPE_E': 'sum','REQUEST_TYPE_N':'sum'})\n", "data_daily = EDA.get_data(table_name='Daily')\n", "data_weekly = EDA.get_data(table_name='Weekly')\n", "if not skipEDA:\n", "    dataframe_names = ['Daily', 'Weekly']\n", "    plotvars = ['REQUEST_TYPE_E', 'REQUEST_TYPE_N','REQUEST_TYPE_TOTAL']\n", "    EDA.plot_from_dataframe(dataframe_names=dataframe_names, plotvarsy=plotvars, kind='line', log_transform=False, suffix='REQ', alpha=0.9)\n", "    EDA.plot_from_dataframe(dataframe_names=dataframe_names, plotvarsy=plotvars, kind='line', log_transform=True, suffix='REQ', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Daily_Town'], x='RECEIVED_DATE', y='REQUEST_TYPE_TOTAL', hue='YEAR', kind='line', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Daily_Town'], x='RECEIVED_DATE', y='REQUEST_TYPE_TOTAL', hue='TOWN_NAME', kind='line', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Weekly'], x='WEEK', y='REQUEST_TYPE_TOTAL', hue='YEAR', kind='line', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Weekly'], x='WEEK', y='REQUEST_TYPE_E', hue='YEAR', kind='line', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Weekly'], x='WEEK', y='REQUEST_TYPE_N', hue='YEAR', kind='line', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Daily_Town'], x='TOWN_NAME', y='REQUEST_TYPE_TOTAL', hue='YEAR', kind='bar', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Daily'], x='REQUEST_TYPE_TOTAL', kind='dist', col='YEAR', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Daily'], x='REQUEST_TYPE_E', kind='dist', col='YEAR', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Daily'], x='REQUEST_TYPE_N', kind='dist', col='YEAR', alpha=0.9)\n", "    EDA.plot_by_category(dataframe_names=['Daily_Town'], x='REQUEST_TYPE_TOTAL', y='REQUEST_TYPE_E', kind='line',hue='YEAR', alpha=0.9)\n", "    print('Pearson Correlation (between E vs N) for Daily Data', EDA.pearsonr(df_name='Daily'))\n", "    print('Pearson Correlation (between E vs N) for Daily Data', EDA.pearsonr(df_name='Weekly'))\n", "    EDA.plot_by_category(dataframe_names=dataframe_names, x='REQUEST_TYPE_E',\n", "                         y='REQUEST_TYPE_N', kind='regplot', alpha=0.9)\n", "    EDA.plot_sns_facet_grid(df_name='Daily', facet_row='YEAR',\n", "                            x='REQUEST_TYPE_N', y='REQUEST_TYPE_E',\n", "                            hue=None, sns_plot_obj=sns.regplot,\n", "                            figname='face_plot_regplot_E_vs_N_by_Year')\n\n", "    ###\n", "    '''\n", "    Check for correlation between Normal and Emergency calls; \n", "    expectaion is to have an imbalance class for\n", "    sufficiently large number of calls and for very small calls we may see 1:1 relation.\n", "    Later stationarity and seaonsal variations shed more light on statistical properties\n", "    '''\n", "    EDA.plot_from_dataframe(dataframe_names=dataframe_names,plotvarsy='REQUEST_TYPE_E',\n", "                            plotvarsx='REQUEST_TYPE_N',kind='scatter',alpha=0.9, subplots=False)\n", "    EDA.plot_from_dataframe(dataframe_names=['Town'], plotvarsy=plotvars, kind='bar', alpha=0.9, suffix='vardist')\n", "    pearsons_corr = EDA.pearsonr(df_name = 'Town', col_name_1 = 'REQUEST_TYPE_N', col_name_2 = 'REQUEST_TYPE_E')\n", "    print(f'Corelation coefficient between Normal and Emergency calls observed in the daily frequency: ', pearsons_corr)\n\n", "    ###Plot rolling averages\n", "    lags = [180, 26]\n", "    df_names = ['Daily', 'Weekly']\n", "    prop = ['REQUEST_TYPE_TOTAL']\n", "    for lag, df_name in zip(lags,df_names):\n", "        for name in prop:\n", "            EDA.plot_moving_average(df_name=df_name, col_name=name, windows=[7,30])\n", "            EDA.plot_exponential_smoothing(df_name=df_name, col_name=name, alphas=[0.7, 0.9])\n", "            EDA.plot_auto_correlations(df_name=df_name, col_name=name, lags=lag)\n", "            if df_name == 'Subdaily': continue\n", "            EDA.plot_ETS(df_name=df_name, col_name=name, method='additive')\n", "    for df_name in ['Daily', 'Weekly']:\n", "        EDA.plot_DoubleExpontentialSmoothing(df_name=df_name, cols=plotvars, trend='add', seasonal_periods=2,\n", "                                             include_seasonal=True)\n\n", "    ### Statistical testing for stationarity\n", "    '''\n", "    For practical purposes we can assume the series to be stationary \n", "    if it has constant statistical properties over time, ie. the following:\n", "    constant mean, constant variance, an autocovariance that does not depend on time.\n", "    Augmented Dickey fuller test conducts a univariate hypothesis testing on the time series \n", "    to determine if the series is stationary. Here the HO is that the series is non-stationary \n", "    (unit root i.e.value of a =1)). If p < 0.05, then the series is stationary. \n", "    '''\n", "    for df_name, window in zip(['Daily', 'Weekly'],[7, 4]):\n", "        EDA.stationarity_test(df_name, cols=['REQUEST_TYPE_E', 'REQUEST_TYPE_N'], window=window)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clip & Transform Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_daily[data_daily['REQUEST_TYPE_E'] > 80].REQUEST_TYPE_E = 80\n", "data_weekly[data_weekly['REQUEST_TYPE_E'] > 400].REQUEST_TYPE_E = 400\n", "if LogTranform:\n", "    for col in ['REQUEST_TYPE_E','REQUEST_TYPE_TOTAL', 'REQUEST_TYPE_N']:\n", "        data_daily[col] = np.log(data_daily[col])\n", "        data_weekly[col] = np.log(data_weekly[col])\n", "###endregion Exploratory Data Analysis"]}, {"cell_type": "markdown", "metadata": {}, "source": ["egion Model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "The origian data has 0.5M entries. Unfortunately, the data as such could not be used <br>\n", "for the model building method. This is because if we drill down to request by town we would end up with<br>\n", "handful of data points that are not sufficient for prediction. It is possible to treat this as a classification problem <br>\n", "if we have lot more data per town. Due to this, the data is being agregated by date and town information is discarded. <br>\n", "So the prediction is being performed by date and treated as a multivariate time series problem. <br>\n", "Not all models do a good job in this case but the following models have been explored. Amongst the different approaches,<br>\n", "Prophet (facebook's time series forecasting at scale) is the most straightforward and reasonable. It can also handle <br>\n", "multiple variables (one could add regressions as constraints) so that dependency between predictors/output is captured.<br>\n", "For additional reading, refer: https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#additional-regressors <br>\n", "Multiple models for training, testing and forecasting available. <br>\n", "Models have been tested with varying degrees of predictability.<br>\n", "Tuned models have hyperparameters stored<br>\n", "Untuned models are under <br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if not skipModel:\n", "    params_prophet = {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 30.0,\n", "                            'n_changepoints': 100, 'daily_seasonality': False,\n", "                            'weekly_seasonality': True, 'yearly_seasonality': False} # tuned params\n", "    params_RNN = {'num_lstm_units':100, 'ts_generator_period':7, 'epochs':30, 'batch_size':1,\n", "                  'num_hidden_lstm_units':[100], 'num_hidden_lstm_layers':1, 'lr':0.01,\n", "                  'num_hidden_dense_units':[60], 'num_hidden_dense_layers':1} # not tuned -> need to work on\n", "    params_ML = {'period':5, 'max_depth': 20, 'n_estimators': 20} # tuned params\n", "    # name: Prophet, RNN, ML, SARIMA, VAR (doens't work well for non-stationary),\n", "    name = 'ML'\n", "    tune_params = False\n", "    cols = ['REQUEST_TYPE_TOTAL','REQUEST_TYPE_E']\n", "    data_list = [data_daily]\n", "    periods = [365]\n", "    freqs = ['D']\n", "    params_list = [params_ML]\n", "    for data, period, freq, params in zip(data_list, periods, freqs, params_list):\n", "        model = Model(name = name, cols = cols, data=data,\n", "                      split_time_str ='20150101', end_time_str = '20151201',\n", "                      future_period = period, freq=freq, params=params,\n", "                      resample=False, resample_freq='1H')\n", "        if tune_params: model.hyper_parameter_tuning()\n", "        model.fit()\n", "        # model.cross_validation()\n", "        model.predict()\n", "        model.plot()\n", "        model.evaluate()\n", "#endregion Model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Tuned params:<br>\n", "params_weekly_prophet = {'changepoint_prior_scale': 30.0, 'seasonality_prior_scale': 30.0,<br>\n", "                         'n_changepoints': 25, 'daily_seasonality': False,<br>\n", "                         'weekly_seasonality': True, 'yearly_seasonality': False} # tuned params"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Untuned Params:<br>\n", "parameters_SARIMA = {'p':2, 'd': 1, 'q': 3, 'seasonal_order':((1, 0, 1, 12))} # not tuned -> need to work on<br>\n", "parameters_VAR = {'maxlag':15} # not tuned -> need to work on<br>\n", "params_RNN = {'num_lstm_units':100, 'ts_generator_period':7, 'epochs':30, 'batch_size':1,<br>\n", "                  'num_hidden_lstm_units':[100], 'num_hidden_lstm_layers':1, 'lr':0.01,<br>\n", "                  'num_hidden_dense_units':[60], 'num_hidden_dense_layers':1} # not tuned -> need to work on"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}